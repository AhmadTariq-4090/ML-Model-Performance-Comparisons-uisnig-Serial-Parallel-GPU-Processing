{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n",
        "import joblib\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load dataset from CSV file\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the CSV file\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame containing the dataset\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from {file_path}...\")\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "        print(f\"Dataset shape: {data.shape}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def identify_column_types(df):\n",
        "    \"\"\"\n",
        "    Identify numeric and categorical columns\n",
        "\n",
        "    Args:\n",
        "        df: pandas DataFrame\n",
        "\n",
        "    Returns:\n",
        "        tuple of (numeric_columns, categorical_columns)\n",
        "    \"\"\"\n",
        "    # Assuming the target column is named 'target' - adjust if needed\n",
        "    feature_cols = [col for col in df.columns if col != 'target']\n",
        "\n",
        "    numeric_cols = df[feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_cols = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    print(f\"Numeric features: {numeric_cols}\")\n",
        "    print(f\"Categorical features: {categorical_cols}\")\n",
        "\n",
        "    return numeric_cols, categorical_cols\n",
        "\n",
        "\n",
        "def create_preprocessing_pipeline(numeric_cols, categorical_cols):\n",
        "    \"\"\"\n",
        "    Create a preprocessing pipeline for numeric and categorical features\n",
        "\n",
        "    Args:\n",
        "        numeric_cols: List of numeric column names\n",
        "        categorical_cols: List of categorical column names\n",
        "\n",
        "    Returns:\n",
        "        ColumnTransformer preprocessing pipeline\n",
        "    \"\"\"\n",
        "    # Numeric features pipeline: impute missing values and scale\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Categorical features pipeline: impute missing values and one-hot encode\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Combine preprocessing steps\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ],\n",
        "        remainder='drop'  # Drop any columns not specified\n",
        "    )\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "def build_and_train_model(X_train, y_train, preprocessor):\n",
        "    \"\"\"\n",
        "    Build and train a logistic regression model without parallel processing\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        y_train: Training target\n",
        "        preprocessor: Data preprocessing pipeline\n",
        "\n",
        "    Returns:\n",
        "        Trained pipeline\n",
        "    \"\"\"\n",
        "    # Create the full pipeline with preprocessing and model\n",
        "    model_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
        "    ])\n",
        "\n",
        "    # Define hyperparameters for grid search\n",
        "    param_grid = {\n",
        "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'classifier__solver': ['liblinear', 'saga'],\n",
        "        'classifier__penalty': ['l1', 'l2']\n",
        "    }\n",
        "\n",
        "    # Use GridSearchCV without parallel processing\n",
        "    grid_search = GridSearchCV(\n",
        "        model_pipeline,\n",
        "        param_grid,\n",
        "        cv=5,\n",
        "        scoring='f1',\n",
        "        verbose=1\n",
        "    )\n",
        "\n",
        "    print(\"\\nTraining model with GridSearchCV...\")\n",
        "    grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    return grid_search\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the model on test data\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        X_test: Test features\n",
        "        y_test: Test target\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with evaluation metrics\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating model on test data...\")\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # Print evaluation results\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'confusion_matrix': conf_matrix,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "\n",
        "def main(data_path='data.csv'):\n",
        "    \"\"\"\n",
        "    Main function to run the entire pipeline\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to the CSV file\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Load data\n",
        "    data = load_data(data_path)\n",
        "\n",
        "    # Assuming the target column is named 'target'\n",
        "    target_col = 'target'\n",
        "    X = data.drop(columns=[target_col])\n",
        "    y = data[target_col]\n",
        "\n",
        "    # Identify column types\n",
        "    numeric_cols, categorical_cols = identify_column_types(data)\n",
        "\n",
        "    # Create preprocessing pipeline\n",
        "    preprocessor = create_preprocessing_pipeline(numeric_cols, categorical_cols)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"Training set size: {X_train.shape}\")\n",
        "    print(f\"Test set size: {X_test.shape}\")\n",
        "\n",
        "    # Build and train the model\n",
        "    model = build_and_train_model(X_train, y_train, preprocessor)\n",
        "\n",
        "    # Evaluate the model\n",
        "    metrics = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    # Calculate and print the total execution time\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    print(f\"\\nTotal execution time: {total_time:.2f} seconds\")\n",
        "\n",
        "    # Save the model (optional)\n",
        "    # joblib.dump(model, 'logistic_regression_model.joblib')\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the data path if needed\n",
        "    main(data_path='pdc_dataset_with_target.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MGX-MMyM-NvQ",
        "outputId": "e50f5310-63fc-4fc5-e711-f4fd7cbda461"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from pdc_dataset_with_target.csv...\n",
            "Dataset shape: (41000, 8)\n",
            "Numeric features: ['feature_1', 'feature_2', 'feature_4', 'feature_6', 'feature_7']\n",
            "Categorical features: ['feature_3', 'feature_5']\n",
            "Training set size: (32800, 7)\n",
            "Test set size: (8200, 7)\n",
            "\n",
            "Training model with GridSearchCV...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
            "Best cross-validation score: 0.0012\n",
            "\n",
            "Evaluating model on test data...\n",
            "\n",
            "Accuracy: 0.6020\n",
            "F1 Score: 0.0012\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4934    2]\n",
            " [3262    2]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      1.00      0.75      4936\n",
            "           1       0.50      0.00      0.00      3264\n",
            "\n",
            "    accuracy                           0.60      8200\n",
            "   macro avg       0.55      0.50      0.38      8200\n",
            "weighted avg       0.56      0.60      0.45      8200\n",
            "\n",
            "\n",
            "Total execution time: 134.44 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NgycHB58M3XT",
        "outputId": "2a17e054-f8c1-48b2-898d-184067318b98"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running with 2 CPU cores\n",
            "Loading data from pdc_dataset_with_target.csv...\n",
            "Dataset shape: (41000, 8)\n",
            "Numeric features: ['feature_1', 'feature_2', 'feature_4', 'feature_6', 'feature_7']\n",
            "Categorical features: ['feature_3', 'feature_5']\n",
            "Training set size: (32800, 7)\n",
            "Test set size: (8200, 7)\n",
            "\n",
            "Training model with GridSearchCV and parallel processing...\n",
            "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best parameters: {'classifier__C': 0.001, 'classifier__penalty': 'l2', 'classifier__solver': 'liblinear'}\n",
            "Best cross-validation score: 0.0012\n",
            "\n",
            "Evaluating model on test data...\n",
            "\n",
            "Accuracy: 0.6020\n",
            "F1 Score: 0.0012\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4934    2]\n",
            " [3262    2]]\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.60      1.00      0.75      4936\n",
            "           1       0.50      0.00      0.00      3264\n",
            "\n",
            "    accuracy                           0.60      8200\n",
            "   macro avg       0.55      0.50      0.38      8200\n",
            "weighted avg       0.56      0.60      0.45      8200\n",
            "\n",
            "\n",
            "Total execution time: 126.73 seconds\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_sag.py:348: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:1271: UserWarning: 'n_jobs' > 1 does not have any effect when 'solver' is set to 'liblinear'. Got 'n_jobs' = 2.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n",
        "import joblib\n",
        "from joblib import parallel_backend\n",
        "\n",
        "\n",
        "def load_data(file_path):\n",
        "    \"\"\"\n",
        "    Load dataset from CSV file\n",
        "\n",
        "    Args:\n",
        "        file_path: Path to the CSV file\n",
        "\n",
        "    Returns:\n",
        "        pandas DataFrame containing the dataset\n",
        "    \"\"\"\n",
        "    print(f\"Loading data from {file_path}...\")\n",
        "    try:\n",
        "        data = pd.read_csv(file_path)\n",
        "        print(f\"Dataset shape: {data.shape}\")\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading dataset: {e}\")\n",
        "        raise\n",
        "\n",
        "\n",
        "def identify_column_types(df):\n",
        "    \"\"\"\n",
        "    Identify numeric and categorical columns\n",
        "\n",
        "    Args:\n",
        "        df: pandas DataFrame\n",
        "\n",
        "    Returns:\n",
        "        tuple of (numeric_columns, categorical_columns)\n",
        "    \"\"\"\n",
        "    # Assuming the target column is named 'target' - adjust if needed\n",
        "    feature_cols = [col for col in df.columns if col != 'target']\n",
        "\n",
        "    numeric_cols = df[feature_cols].select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_cols = df[feature_cols].select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    print(f\"Numeric features: {numeric_cols}\")\n",
        "    print(f\"Categorical features: {categorical_cols}\")\n",
        "\n",
        "    return numeric_cols, categorical_cols\n",
        "\n",
        "\n",
        "def create_preprocessing_pipeline(numeric_cols, categorical_cols):\n",
        "    \"\"\"\n",
        "    Create a preprocessing pipeline for numeric and categorical features\n",
        "\n",
        "    Args:\n",
        "        numeric_cols: List of numeric column names\n",
        "        categorical_cols: List of categorical column names\n",
        "\n",
        "    Returns:\n",
        "        ColumnTransformer preprocessing pipeline\n",
        "    \"\"\"\n",
        "    # Numeric features pipeline: impute missing values and scale\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    # Categorical features pipeline: impute missing values and one-hot encode\n",
        "    categorical_transformer = Pipeline(steps=[\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
        "    ])\n",
        "\n",
        "    # Combine preprocessing steps\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_cols),\n",
        "            ('cat', categorical_transformer, categorical_cols)\n",
        "        ],\n",
        "        remainder='drop'  # Drop any columns not specified\n",
        "    )\n",
        "\n",
        "    return preprocessor\n",
        "\n",
        "\n",
        "def build_and_train_model(X_train, y_train, preprocessor, n_jobs=-1):\n",
        "    \"\"\"\n",
        "    Build and train a logistic regression model with parallel processing\n",
        "\n",
        "    Args:\n",
        "        X_train: Training features\n",
        "        y_train: Training target\n",
        "        preprocessor: Data preprocessing pipeline\n",
        "        n_jobs: Number of parallel jobs (-1 for all available cores)\n",
        "\n",
        "    Returns:\n",
        "        Trained pipeline\n",
        "    \"\"\"\n",
        "    # Create the full pipeline with preprocessing and model\n",
        "    model_pipeline = Pipeline(steps=[\n",
        "        ('preprocessor', preprocessor),\n",
        "        ('classifier', LogisticRegression(max_iter=1000, random_state=42))\n",
        "    ])\n",
        "\n",
        "    # Define hyperparameters for grid search\n",
        "    param_grid = {\n",
        "        'classifier__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
        "        'classifier__solver': ['liblinear', 'saga'],\n",
        "        'classifier__penalty': ['l1', 'l2']\n",
        "    }\n",
        "\n",
        "    # Use GridSearchCV with parallel processing\n",
        "    with parallel_backend('threading', n_jobs=n_jobs):\n",
        "        grid_search = GridSearchCV(\n",
        "            model_pipeline,\n",
        "            param_grid,\n",
        "            cv=5,\n",
        "            scoring='f1',\n",
        "            n_jobs=n_jobs,\n",
        "            verbose=1\n",
        "        )\n",
        "\n",
        "        print(\"\\nTraining model with GridSearchCV and parallel processing...\")\n",
        "        grid_search.fit(X_train, y_train)\n",
        "\n",
        "    print(f\"Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")\n",
        "\n",
        "    return grid_search\n",
        "\n",
        "\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    \"\"\"\n",
        "    Evaluate the model on test data\n",
        "\n",
        "    Args:\n",
        "        model: Trained model\n",
        "        X_test: Test features\n",
        "        y_test: Test target\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with evaluation metrics\n",
        "    \"\"\"\n",
        "    print(\"\\nEvaluating model on test data...\")\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "    # Print evaluation results\n",
        "    print(f\"\\nAccuracy: {accuracy:.4f}\")\n",
        "    print(f\"F1 Score: {f1:.4f}\")\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(conf_matrix)\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "\n",
        "    return {\n",
        "        'accuracy': accuracy,\n",
        "        'confusion_matrix': conf_matrix,\n",
        "        'f1_score': f1\n",
        "    }\n",
        "\n",
        "\n",
        "def main(data_path='data.csv'):\n",
        "    \"\"\"\n",
        "    Main function to run the entire pipeline\n",
        "\n",
        "    Args:\n",
        "        data_path: Path to the CSV file\n",
        "    \"\"\"\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Set the number of CPU cores to use (-1 for all available)\n",
        "    n_cores = joblib.cpu_count()\n",
        "    print(f\"Running with {n_cores} CPU cores\")\n",
        "\n",
        "    # Load data\n",
        "    data = load_data(data_path)\n",
        "\n",
        "    # Assuming the target column is named 'target'\n",
        "    target_col = 'target'\n",
        "    X = data.drop(columns=[target_col])\n",
        "    y = data[target_col]\n",
        "\n",
        "    # Identify column types\n",
        "    numeric_cols, categorical_cols = identify_column_types(data)\n",
        "\n",
        "    # Create preprocessing pipeline\n",
        "    preprocessor = create_preprocessing_pipeline(numeric_cols, categorical_cols)\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"Training set size: {X_train.shape}\")\n",
        "    print(f\"Test set size: {X_test.shape}\")\n",
        "\n",
        "    # Build and train the model\n",
        "    model = build_and_train_model(X_train, y_train, preprocessor, n_jobs=n_cores)\n",
        "\n",
        "    # Evaluate the model\n",
        "    metrics = evaluate_model(model, X_test, y_test)\n",
        "\n",
        "    # Calculate and print the total execution time\n",
        "    end_time = time.time()\n",
        "    total_time = end_time - start_time\n",
        "    print(f\"\\nTotal execution time: {total_time:.2f} seconds\")\n",
        "\n",
        "    # Save the model (optional)\n",
        "    # joblib.dump(model, 'logistic_regression_cpu_model.joblib')\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # You can change the data path if needed\n",
        "    main(data_path='pdc_dataset_with_target.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n8av3coY5GN8",
        "outputId": "c33b9c94-e2f5-416d-a7a4-5e6e00155190"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ GPU available: 1 device(s)\n",
            "TensorFlow 2.18.0, Policy: <DTypePolicy \"mixed_float16\">\n",
            "\n",
            "Train shape: (32800, 10), Test shape: (8200, 10)\n",
            "Epoch 1/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 10ms/step - accuracy: 0.5137 - loss: 0.7667 - val_accuracy: 0.5206 - val_loss: 0.7381\n",
            "Epoch 2/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5362 - loss: 0.7232 - val_accuracy: 0.5410 - val_loss: 0.7094\n",
            "Epoch 3/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.5558 - loss: 0.6992 - val_accuracy: 0.5655 - val_loss: 0.6927\n",
            "Epoch 4/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.5726 - loss: 0.6857 - val_accuracy: 0.5809 - val_loss: 0.6834\n",
            "Epoch 5/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5942 - loss: 0.6784 - val_accuracy: 0.5917 - val_loss: 0.6782\n",
            "Epoch 6/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.5999 - loss: 0.6747 - val_accuracy: 0.6013 - val_loss: 0.6756\n",
            "Epoch 7/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6023 - loss: 0.6729 - val_accuracy: 0.6020 - val_loss: 0.6742\n",
            "Epoch 8/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6025 - loss: 0.6721 - val_accuracy: 0.6018 - val_loss: 0.6735\n",
            "Epoch 9/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6026 - loss: 0.6718 - val_accuracy: 0.6018 - val_loss: 0.6732\n",
            "Epoch 10/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6026 - loss: 0.6717 - val_accuracy: 0.6020 - val_loss: 0.6730\n",
            "Epoch 11/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6027 - loss: 0.6716 - val_accuracy: 0.6020 - val_loss: 0.6730\n",
            "Epoch 12/50\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6027 - loss: 0.6716 - val_accuracy: 0.6020 - val_loss: 0.6729\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\n",
            "Results:\n",
            " Accuracy: 0.6020\n",
            " F1 Score: 0.0012\n",
            "Confusion Matrix : [[4934    2]\n",
            " [3262    2]]\n",
            "Training Time: 7.29s, Evaluation Time: 0.06s\n",
            "Total Elapsed: 7.77s\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import time\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import mixed_precision\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, classification_report\n",
        "\n",
        "# Enable mixed precision for faster computation on modern GPUs\n",
        "mixed_precision.set_global_policy('mixed_float16')\n",
        "\n",
        "# GPU check\n",
        "def check_gpu():\n",
        "    gpus = tf.config.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        for gpu in gpus:\n",
        "            tf.config.experimental.set_memory_growth(gpu, True)\n",
        "        print(f\"✅ GPU available: {len(gpus)} device(s)\")\n",
        "    else:\n",
        "        print(\"⚠️ No GPU found. Using CPU.\")\n",
        "    return bool(gpus)\n",
        "\n",
        "# Run GPU check at start\n",
        "check_gpu()\n",
        "print(f\"TensorFlow {tf.__version__}, Policy: {mixed_precision.global_policy()}\\n\")\n",
        "\n",
        "def load_and_preprocess(file_path):\n",
        "    data = pd.read_csv(file_path)\n",
        "    target_col = 'target'\n",
        "    X = data.drop(columns=[target_col])\n",
        "    y = data[target_col].values\n",
        "\n",
        "    # Identify column types\n",
        "    numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "    categorical_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
        "\n",
        "    # Build preprocessing pipelines\n",
        "    num_pipe = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='median')),\n",
        "        ('scaler', StandardScaler())\n",
        "    ])\n",
        "\n",
        "    cat_pipe = Pipeline([\n",
        "        ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "        ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
        "    ])\n",
        "\n",
        "    preproc = ColumnTransformer([\n",
        "        ('num', num_pipe, numeric_cols),\n",
        "        ('cat', cat_pipe, categorical_cols)\n",
        "    ], remainder='drop')\n",
        "\n",
        "    # Fit-transform and split\n",
        "    X_processed = preproc.fit_transform(X)\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X_processed, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "    return (X_train, y_train), (X_test, y_test)\n",
        "\n",
        "\n",
        "def make_tf_dataset(X, y, batch_size=256, shuffle=False):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((X, y))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(X))\n",
        "    return ds.batch(batch_size).cache().prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "\n",
        "def build_model(input_dim):\n",
        "    model = Sequential([\n",
        "        Input(shape=(input_dim,)),\n",
        "        Dense(1, activation='sigmoid', dtype='float32')  # Logistic regression\n",
        "    ])\n",
        "    model.compile(\n",
        "        optimizer=Adam(learning_rate=0.001),\n",
        "        loss='binary_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "        jit_compile=True\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "def main(data_path='data.csv'):\n",
        "    start = time.time()\n",
        "\n",
        "    # Load & preprocess\n",
        "    (X_train, y_train), (X_test, y_test) = load_and_preprocess(data_path)\n",
        "    print(f\"Train shape: {X_train.shape}, Test shape: {X_test.shape}\")\n",
        "\n",
        "    # Create tf.data datasets\n",
        "    train_ds = make_tf_dataset(X_train, y_train, shuffle=True)\n",
        "    test_ds = make_tf_dataset(X_test, y_test)\n",
        "\n",
        "    # Build & train\n",
        "    model = build_model(X_train.shape[1])\n",
        "    es = EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True)\n",
        "    train_time_start = time.time()\n",
        "    model.fit(\n",
        "        train_ds,\n",
        "        validation_data=test_ds,\n",
        "        epochs=50,\n",
        "        callbacks=[es],\n",
        "        verbose=1\n",
        "    )\n",
        "    train_time = time.time() - train_time_start\n",
        "\n",
        "    # Evaluate\n",
        "    eval_start = time.time()\n",
        "    loss, acc = model.evaluate(test_ds, verbose=0)\n",
        "    eval_time = time.time() - eval_start\n",
        "    y_pred = (model.predict(test_ds) > 0.5).astype(int).flatten()\n",
        "\n",
        "    # Compute metrics\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "    print(f\"\\nResults:\\n Accuracy: {acc:.4f}\\n F1 Score: {f1:.4f}\")\n",
        "    print(\"Confusion Matrix :\", cm)\n",
        "    print(f\"Training Time: {train_time:.2f}s, Evaluation Time: {eval_time:.2f}s\")\n",
        "    print(f\"Total Elapsed: {time.time() - start:.2f}s\")\n",
        "\n",
        "    return {'accuracy': acc, 'f1_score': f1, 'confusion_matrix': cm}\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main('pdc_dataset_with_target.csv')\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}